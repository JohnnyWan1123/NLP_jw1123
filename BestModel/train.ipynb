{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: PCL Detection with DeBERTa-v3-base\n",
    "\n",
    "This notebook implements the proposed approach from Exercise 3:\n",
    "1. **Stage A** — Data Preparation: load dataset, binarise labels, train/dev split\n",
    "2. **Stage B** — Model Training: fine-tune DeBERTa-v3-base with class-weighted cross-entropy\n",
    "3. **Stage C** — Threshold Optimisation: grid search for optimal classification threshold\n",
    "\n",
    "**Output:** `dev.txt` and `test.txt` prediction files, saved model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc05bd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    classification_report,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a6fd27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  model_name: microsoft/deberta-v3-base\n",
      "  max_length: 128\n",
      "  batch_size: 16\n",
      "  learning_rate: 2e-05\n",
      "  weight_decay: 0.01\n",
      "  num_epochs: 10\n",
      "  warmup_ratio: 0.1\n",
      "  patience: 3\n",
      "  seed: 42\n",
      "  threshold_lo: 0.3\n",
      "  threshold_hi: 0.7\n",
      "  threshold_step: 0.01\n"
     ]
    }
   ],
   "source": [
    "# ---- All hyperparameters in one place ----\n",
    "CONFIG = {\n",
    "    \"model_name\": \"microsoft/deberta-v3-base\",\n",
    "    \"max_length\": 128,\n",
    "    \"batch_size\": 16,\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"num_epochs\": 10,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"patience\": 3,\n",
    "    \"seed\": SEED,\n",
    "    \"threshold_lo\": 0.30,\n",
    "    \"threshold_hi\": 0.70,\n",
    "    \"threshold_step\": 0.01,\n",
    "}\n",
    "\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d70a325",
   "metadata": {},
   "source": [
    "## Stage A: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "288db942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Official split par_ids — Train: 8375, Dev: 2094\n",
      "\n",
      "Total labelled samples: 10469\n",
      "PCL:     993 (9.5%)\n",
      "Not PCL: 9476 (90.5%)\n",
      "Test samples: 3832\n"
     ]
    }
   ],
   "source": [
    "# Resolve data path (works from BestModel/ or repo root)\n",
    "DATA_FILE = \"dontpatronizeme_pcl.tsv\"\n",
    "TRAIN_SPLIT_FILE = \"train_semeval_parids-labels.csv\"\n",
    "DEV_SPLIT_FILE = \"dev_semeval_parids-labels.csv\"\n",
    "TEST_FILE = \"task4_test.tsv\"\n",
    "\n",
    "def find_file(name):\n",
    "    if os.path.exists(name):\n",
    "        return name\n",
    "    if os.path.exists(os.path.join(\"..\", name)):\n",
    "        return os.path.join(\"..\", name)\n",
    "    raise FileNotFoundError(f\"{name} not found in current or parent directory.\")\n",
    "\n",
    "DATA_PATH = find_file(DATA_FILE)\n",
    "TRAIN_SPLIT_PATH = find_file(TRAIN_SPLIT_FILE)\n",
    "DEV_SPLIT_PATH = find_file(DEV_SPLIT_FILE)\n",
    "TEST_PATH = find_file(TEST_FILE)\n",
    "\n",
    "# ---- Load main dataset ----\n",
    "df = pd.read_csv(\n",
    "    DATA_PATH,\n",
    "    sep=\"\\t\",\n",
    "    skiprows=4,\n",
    "    header=None,\n",
    "    names=[\"par_id\", \"art_id\", \"keyword\", \"country_code\", \"text\", \"label\"],\n",
    ")\n",
    "\n",
    "# Binarise: 0-1 -> Not PCL (0), 2-4 -> PCL (1)\n",
    "df[\"binary_label\"] = (df[\"label\"] >= 2).astype(int)\n",
    "df[\"text\"] = df[\"text\"].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "# ---- Load official SemEval train/dev splits ----\n",
    "train_split = pd.read_csv(TRAIN_SPLIT_PATH)\n",
    "dev_split = pd.read_csv(DEV_SPLIT_PATH)\n",
    "print(f\"Official split par_ids — Train: {len(train_split)}, Dev: {len(dev_split)}\")\n",
    "\n",
    "# ---- Load test set ----\n",
    "test_df = pd.read_csv(\n",
    "    TEST_PATH,\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"test_id\", \"art_id\", \"keyword\", \"country_code\", \"text\"],\n",
    ")\n",
    "test_df[\"text\"] = test_df[\"text\"].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "n_total = len(df)\n",
    "n_pcl = df[\"binary_label\"].sum()\n",
    "print(f\"\\nTotal labelled samples: {n_total}\")\n",
    "print(f\"PCL:     {n_pcl} ({n_pcl/n_total*100:.1f}%)\")\n",
    "print(f\"Not PCL: {n_total - n_pcl} ({(n_total - n_pcl)/n_total*100:.1f}%)\")\n",
    "print(f\"Test samples: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed0dba1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 8375 matched out of 8375 split IDs  (PCL 794)\n",
      "Dev:   2094 matched out of 2094 split IDs  (PCL 199)\n"
     ]
    }
   ],
   "source": [
    "# Use official SemEval train/dev splits (join on par_id)\n",
    "train_par_ids = set(train_split[\"par_id\"].values)\n",
    "dev_par_ids = set(dev_split[\"par_id\"].values)\n",
    "\n",
    "train_df = df[df[\"par_id\"].isin(train_par_ids)].reset_index(drop=True)\n",
    "dev_df = df[df[\"par_id\"].isin(dev_par_ids)].reset_index(drop=True)\n",
    "\n",
    "# Sanity check: ensure all par_ids were matched\n",
    "n_train_matched = len(train_df)\n",
    "n_dev_matched = len(dev_df)\n",
    "print(f\"Train: {n_train_matched} matched out of {len(train_split)} split IDs  (PCL {train_df['binary_label'].sum()})\")\n",
    "print(f\"Dev:   {n_dev_matched} matched out of {len(dev_split)} split IDs  (PCL {dev_df['binary_label'].sum()})\")\n",
    "\n",
    "if n_train_matched != len(train_split) or n_dev_matched != len(dev_split):\n",
    "    print(\"WARNING: Some par_ids from the split files were not found in the dataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fac7c7",
   "metadata": {},
   "source": [
    "## Stage B: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd4b9dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenising train...\n",
      "Tokenising dev...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# ---- Tokenisation ----\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"])\n",
    "\n",
    "\n",
    "def tokenize(texts, max_length):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"Tokenising train...\")\n",
    "train_enc = tokenize(train_df[\"text\"], CONFIG[\"max_length\"])\n",
    "print(\"Tokenising dev...\")\n",
    "dev_enc = tokenize(dev_df[\"text\"], CONFIG[\"max_length\"])\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a0a5da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 524, Dev batches: 131\n"
     ]
    }
   ],
   "source": [
    "class PCLDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "train_dataset = PCLDataset(train_enc, train_df[\"binary_label\"].values)\n",
    "dev_dataset = PCLDataset(dev_enc, dev_df[\"binary_label\"].values)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True\n",
    ")\n",
    "dev_loader = DataLoader(\n",
    "    dev_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Dev batches: {len(dev_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3cbd7fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 198/198 [00:00<00:00, 362.25it/s, Materializing param=deberta.encoder.rel_embeddings.weight]                     \n",
      "\u001b[1mDebertaV2ForSequenceClassification LOAD REPORT\u001b[0m from: microsoft/deberta-v3-base\n",
      "Key                                     | Status     | \n",
      "----------------------------------------+------------+-\n",
      "mask_predictions.classifier.bias        | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.weight | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.weight     | UNEXPECTED | \n",
      "mask_predictions.LayerNorm.bias         | UNEXPECTED | \n",
      "mask_predictions.classifier.weight      | UNEXPECTED | \n",
      "lm_predictions.lm_head.bias             | UNEXPECTED | \n",
      "lm_predictions.lm_head.LayerNorm.bias   | UNEXPECTED | \n",
      "mask_predictions.dense.bias             | UNEXPECTED | \n",
      "mask_predictions.LayerNorm.weight       | UNEXPECTED | \n",
      "mask_predictions.dense.weight           | UNEXPECTED | \n",
      "lm_predictions.lm_head.dense.bias       | UNEXPECTED | \n",
      "classifier.weight                       | MISSING    | \n",
      "pooler.dense.bias                       | MISSING    | \n",
      "pooler.dense.weight                     | MISSING    | \n",
      "classifier.bias                         | MISSING    | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001b[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: Not-PCL=0.5524, PCL=5.2739\n",
      "Total steps: 5240, Warmup steps: 524\n",
      "Parameters: 184,423,682\n",
      "Model dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# ---- Model ----\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    CONFIG[\"model_name\"], num_labels=2, torch_dtype=torch.float32,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# ---- Class-weighted cross-entropy (inversely proportional to frequency) ----\n",
    "n_train = len(train_df)\n",
    "n_neg = (train_df[\"binary_label\"] == 0).sum()\n",
    "n_pos = (train_df[\"binary_label\"] == 1).sum()\n",
    "w_neg = n_train / (2 * n_neg)\n",
    "w_pos = n_train / (2 * n_pos)\n",
    "class_weights = torch.tensor([w_neg, w_pos], dtype=torch.float32).to(device)\n",
    "print(f\"Class weights: Not-PCL={w_neg:.4f}, PCL={w_pos:.4f}\")\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# ---- Optimiser & scheduler ----\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG[\"learning_rate\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"],\n",
    ")\n",
    "total_steps = len(train_loader) * CONFIG[\"num_epochs\"]\n",
    "warmup_steps = int(CONFIG[\"warmup_ratio\"] * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"Total steps: {total_steps}, Warmup steps: {warmup_steps}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Model dtype: {next(model.parameters()).dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c736add",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    \"\"\"Return (avg_loss, f1, probs_array, labels_array).\"\"\"\n",
    "    model.eval()\n",
    "    all_probs, all_labels = [], []\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in loader:\n",
    "        ids = batch[\"input_ids\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        labs = batch[\"labels\"].to(device)\n",
    "\n",
    "        logits = model(input_ids=ids, attention_mask=mask).logits\n",
    "        running_loss += loss_fn(logits, labs).item()\n",
    "\n",
    "        probs = torch.softmax(logits, dim=-1)[:, 1]\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "        all_labels.extend(labs.cpu().numpy())\n",
    "\n",
    "    all_probs = np.array(all_probs)\n",
    "    all_labels = np.array(all_labels)\n",
    "    preds = (all_probs >= 0.5).astype(int)\n",
    "    f1 = f1_score(all_labels, preds, pos_label=1)\n",
    "    return running_loss / len(loader), f1, all_probs, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069acc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Training loop with early stopping ----\n",
    "best_f1 = 0.0\n",
    "patience_ctr = 0\n",
    "history = {\"train_loss\": [], \"dev_loss\": [], \"dev_f1\": []}\n",
    "\n",
    "for epoch in range(1, CONFIG[\"num_epochs\"] + 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for step, batch in enumerate(train_loader, 1):\n",
    "        ids = batch[\"input_ids\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        labs = batch[\"labels\"].to(device)\n",
    "\n",
    "        logits = model(input_ids=ids, attention_mask=mask).logits\n",
    "        loss = loss_fn(logits, labs)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if step % 10 == 0 or step == 1:\n",
    "            print(f\"  [{epoch}] step {step}/{len(train_loader)}  loss={loss.item():.4f}\")\n",
    "\n",
    "    avg_train = epoch_loss / len(train_loader)\n",
    "    dev_loss, dev_f1, _, _ = evaluate(model, dev_loader)\n",
    "\n",
    "    history[\"train_loss\"].append(avg_train)\n",
    "    history[\"dev_loss\"].append(dev_loss)\n",
    "    history[\"dev_f1\"].append(dev_f1)\n",
    "\n",
    "    tag = \"\"\n",
    "    if dev_f1 > best_f1:\n",
    "        best_f1 = dev_f1\n",
    "        patience_ctr = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        tag = \" *saved*\"\n",
    "    else:\n",
    "        patience_ctr += 1\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch}: train_loss={avg_train:.4f}  \"\n",
    "        f\"dev_loss={dev_loss:.4f}  dev_f1={dev_f1:.4f}  \"\n",
    "        f\"patience={patience_ctr}/{CONFIG['patience']}{tag}\"\n",
    "    )\n",
    "\n",
    "    if patience_ctr >= CONFIG[\"patience\"]:\n",
    "        print(\"Early stopping.\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nBest dev F1 (t=0.5): {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ae979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Training curves ----\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "epochs_range = range(1, len(history[\"train_loss\"]) + 1)\n",
    "ax1.plot(epochs_range, history[\"train_loss\"], \"o-\", label=\"Train\")\n",
    "ax1.plot(epochs_range, history[\"dev_loss\"], \"s-\", label=\"Dev\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_title(\"Loss\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(epochs_range, history[\"dev_f1\"], \"o-\", color=\"green\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"F1\")\n",
    "ax2.set_title(\"Dev F1 (Positive Class)\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_curves.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f9f3b5",
   "metadata": {},
   "source": [
    "## Stage C: Threshold Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d820971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload best checkpoint\n",
    "model.load_state_dict(torch.load(\"best_model.pt\", map_location=device))\n",
    "_, _, dev_probs, dev_labels = evaluate(model, dev_loader)\n",
    "\n",
    "# Grid search\n",
    "thresholds = np.arange(\n",
    "    CONFIG[\"threshold_lo\"],\n",
    "    CONFIG[\"threshold_hi\"] + CONFIG[\"threshold_step\"],\n",
    "    CONFIG[\"threshold_step\"],\n",
    ")\n",
    "\n",
    "best_t, best_t_f1 = 0.5, 0.0\n",
    "records = []\n",
    "for t in thresholds:\n",
    "    preds = (dev_probs >= t).astype(int)\n",
    "    f1 = f1_score(dev_labels, preds, pos_label=1)\n",
    "    prec = precision_score(dev_labels, preds, pos_label=1, zero_division=0)\n",
    "    rec = recall_score(dev_labels, preds, pos_label=1, zero_division=0)\n",
    "    records.append({\"t\": t, \"f1\": f1, \"precision\": prec, \"recall\": rec})\n",
    "    if f1 > best_t_f1:\n",
    "        best_t_f1 = f1\n",
    "        best_t = t\n",
    "\n",
    "res = pd.DataFrame(records)\n",
    "print(f\"Optimal threshold: {best_t:.2f}\")\n",
    "print(f\"F1 at optimal threshold: {best_t_f1:.4f}\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(res[\"t\"], res[\"f1\"], label=\"F1\", linewidth=2)\n",
    "ax.plot(res[\"t\"], res[\"precision\"], \"--\", label=\"Precision\", linewidth=2)\n",
    "ax.plot(res[\"t\"], res[\"recall\"], \"--\", label=\"Recall\", linewidth=2)\n",
    "ax.axvline(best_t, color=\"red\", linestyle=\":\", linewidth=2, label=f\"t*={best_t:.2f}\")\n",
    "ax.set_xlabel(\"Threshold\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Threshold Optimisation on Dev Set\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"threshold_optimisation.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e55597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Final evaluation with optimal threshold ----\n",
    "dev_preds = (dev_probs >= best_t).astype(int)\n",
    "\n",
    "print(f\"Threshold = {best_t:.2f}\")\n",
    "print(\"=\" * 55)\n",
    "print(classification_report(dev_labels, dev_preds, target_names=[\"Not PCL\", \"PCL\"]))\n",
    "\n",
    "f1_final = f1_score(dev_labels, dev_preds, pos_label=1)\n",
    "prec_final = precision_score(dev_labels, dev_preds, pos_label=1)\n",
    "rec_final = recall_score(dev_labels, dev_preds, pos_label=1)\n",
    "print(f\"PCL Precision: {prec_final:.4f}\")\n",
    "print(f\"PCL Recall:    {rec_final:.4f}\")\n",
    "print(f\"PCL F1:        {f1_final:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e240bd",
   "metadata": {},
   "source": [
    "## Save Predictions & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5756ccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- dev.txt: predictions on official dev set ----\n",
    "dev_pred_path = os.path.join(\"..\", \"dev.txt\")\n",
    "with open(dev_pred_path, \"w\") as f:\n",
    "    for p in dev_preds:\n",
    "        f.write(f\"{p}\\n\")\n",
    "print(f\"Saved {len(dev_preds)} dev predictions -> {dev_pred_path}\")\n",
    "\n",
    "# ---- test.txt: predictions on actual unlabelled test set (task4_test.tsv) ----\n",
    "print(f\"\\nGenerating predictions on test set ({len(test_df)} samples)...\")\n",
    "test_enc = tokenize(test_df[\"text\"], CONFIG[\"max_length\"])\n",
    "test_ds = PCLDataset(test_enc, np.zeros(len(test_df), dtype=int))  # dummy labels\n",
    "test_loader = DataLoader(test_ds, batch_size=CONFIG[\"batch_size\"], shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "all_test_probs = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        ids = batch[\"input_ids\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        logits = model(input_ids=ids, attention_mask=mask).logits\n",
    "        probs = torch.softmax(logits, dim=-1)[:, 1]\n",
    "        all_test_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "test_preds = (np.array(all_test_probs) >= best_t).astype(int)\n",
    "test_pred_path = os.path.join(\"..\", \"test.txt\")\n",
    "with open(test_pred_path, \"w\") as f:\n",
    "    for p in test_preds:\n",
    "        f.write(f\"{p}\\n\")\n",
    "print(f\"Saved {len(test_preds)} test predictions -> {test_pred_path}\")\n",
    "print(f\"Test PCL rate: {test_preds.sum()}/{len(test_preds)} ({test_preds.mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Save model & tokenizer for reproducibility ----\n",
    "save_dir = \"saved_model\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "# Save training config + threshold\n",
    "meta = {\n",
    "    **CONFIG,\n",
    "    \"best_threshold\": float(best_t),\n",
    "    \"best_f1\": float(best_t_f1),\n",
    "    \"dev_precision\": float(prec_final),\n",
    "    \"dev_recall\": float(rec_final),\n",
    "}\n",
    "with open(os.path.join(save_dir, \"training_config.json\"), \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(f\"Model saved to {save_dir}/\")\n",
    "print(f\"Best threshold: {best_t:.2f}\")\n",
    "print(f\"Dev F1: {best_t_f1:.4f}\")\n",
    "print(\"\\nDone!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbf8b245",
   "metadata": {},
   "source": [
    "# Exercise 4: PCL Detection with RoBERTa-large\n",
    "\n",
    "This notebook implements the proposed approach from Exercise 3:\n",
    "1. **Stage A** — Data Preparation: load dataset, binarise labels, train/dev split\n",
    "2. **Stage B** — Model Training: fine-tune RoBERTa-large with focal loss, LLRD, and mixed-precision (AMP)\n",
    "3. **Stage C** — Threshold Optimisation: grid search for optimal classification threshold\n",
    "\n",
    "**Output:** `dev.txt` and `test.txt` prediction files, saved model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc05bd79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory set to: /home/azureuser/NLP_jw1123/BestModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (3.0.0)\n",
      "Requirement already satisfied: numpy in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (2.4.2)\n",
      "Requirement already satisfied: matplotlib in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (3.10.8)\n",
      "Requirement already satisfied: nltk in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (3.9.2)\n",
      "Collecting torch==2.5.0\n",
      "  Using cached torch-2.5.0-cp312-cp312-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: transformers in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (5.1.0)\n",
      "Requirement already satisfied: accelerate in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (1.12.0)\n",
      "Requirement already satisfied: scikit-learn in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (1.8.0)\n",
      "Requirement already satisfied: sentencepiece in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (0.2.1)\n",
      "Requirement already satisfied: datasets in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (4.5.0)\n",
      "Requirement already satisfied: tiktoken in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (0.12.0)\n",
      "Requirement already satisfied: protobuf in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (6.33.5)\n",
      "Requirement already satisfied: filelock in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from torch==2.5.0) (3.21.2)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from torch==2.5.0) (4.15.0)\n",
      "Requirement already satisfied: networkx in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from torch==2.5.0) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from torch==2.5.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from torch==2.5.0) (2025.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.0)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.0)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.0)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.0)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.0)\n",
      "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.0)\n",
      "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.0)\n",
      "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.0)\n",
      "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.0)\n",
      "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch==2.5.0)\n",
      "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch==2.5.0)\n",
      "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.0)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.1.0 (from torch==2.5.0)\n",
      "  Using cached triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: setuptools in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from torch==2.5.0) (82.0.0)\n",
      "Collecting sympy==1.13.1 (from torch==2.5.0)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch==2.5.0) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from matplotlib) (26.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from matplotlib) (12.1.1)\n",
      "Requirement already satisfied: pyparsing>=3 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from matplotlib) (3.3.2)\n",
      "Requirement already satisfied: click in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from nltk) (8.3.1)\n",
      "Requirement already satisfied: joblib in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from nltk) (1.5.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from nltk) (2026.1.15)\n",
      "Requirement already satisfied: tqdm in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from nltk) (4.67.3)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from transformers) (1.4.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: typer-slim in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from transformers) (0.23.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: psutil in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from accelerate) (7.2.2)\n",
      "Requirement already satisfied: scipy>=1.10.0 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from scikit-learn) (1.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from datasets) (23.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: certifi in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: shellingham in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.6.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from jinja2->torch==2.5.0) (3.0.3)\n",
      "Requirement already satisfied: typer>=0.23.1 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from typer-slim->transformers) (0.23.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from typer>=0.23.1->typer-slim->transformers) (14.3.2)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from typer>=0.23.1->typer-slim->transformers) (0.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from rich>=10.11.0->typer>=0.23.1->typer-slim->transformers) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from rich>=10.11.0->typer>=0.23.1->typer-slim->transformers) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/azureuser/NLP_jw1123/.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.23.1->typer-slim->transformers) (0.1.2)\n",
      "Using cached torch-2.5.0-cp312-cp312-manylinux1_x86_64.whl (906.4 MB)\n",
      "Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
      "Installing collected packages: triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.6.0\n",
      "    Uninstalling triton-3.6.0:\n",
      "      Successfully uninstalled triton-3.6.0\n",
      "  Attempting uninstall: sympy\n",
      "    Found existing installation: sympy 1.14.0\n",
      "    Uninstalling sympy-1.14.0:\n",
      "      Successfully uninstalled sympy-1.14.0\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.8.90\n",
      "    Uninstalling nvidia-nvtx-cu12-12.8.90:\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.8.90\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n",
      "  Attempting uninstall: nvidia-nccl-cu12\n",
      "    Found existing installation: nvidia-nccl-cu12 2.27.5\n",
      "    Uninstalling nvidia-nccl-cu12-2.27.5:\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.27.5\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.9.90\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.90:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.8.90\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.8.90:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.8.90\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.8.93\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.8.93:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.8.93\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.8.90\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.8.90:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.8.90\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
      "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.10.0\n",
      "    Uninstalling torch-2.10.0:\n",
      "      Successfully uninstalled torch-2.10.0\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 sympy-1.13.1 torch-2.5.0 triton-3.1.0\n"
     ]
    }
   ],
   "source": [
    "# import importlib.util\n",
    "# import sys\n",
    "\n",
    "# # Dictionary mapping import names to pip install names\n",
    "# packages = {\n",
    "#     'pandas': 'pandas',\n",
    "#     'numpy': 'numpy',\n",
    "#     'matplotlib': 'matplotlib',\n",
    "#     'nltk': 'nltk',\n",
    "#     'torch': 'torch',\n",
    "#     'transformers': 'transformers',\n",
    "#     'accelerate': 'accelerate',\n",
    "#     'sklearn': 'scikit-learn',\n",
    "#     'sentencepiece': 'sentencepiece',\n",
    "#     'datasets': 'datasets',\n",
    "#     'tiktoken': 'tiktoken',\n",
    "#     'google.protobuf': 'protobuf'\n",
    "# }\n",
    "\n",
    "# # Check which packages are missing\n",
    "# def is_installed(package):\n",
    "#     try:\n",
    "#         importlib.util.find_spec(package)\n",
    "#         return True\n",
    "#     except ModuleNotFoundError:\n",
    "#         return False\n",
    "\n",
    "# missing_packages = []\n",
    "# for package, pip_name in packages.items():\n",
    "#     if not is_installed(package):\n",
    "#         missing_packages.append(pip_name)\n",
    "\n",
    "# # Install all missing packages in a single pip call\n",
    "# if len(missing_packages) > 0:\n",
    "#     print(f\"Installing missing packages: {missing_packages}\")\n",
    "#     !{sys.executable} -m pip install {' '.join(missing_packages)} --break-system-packages\n",
    "# else:\n",
    "#     print(\"All packages already installed!\")\n",
    "\n",
    "# # Verify installation\n",
    "# print(\"\\nVerifying installations:\")\n",
    "# for package, pip_name in packages.items():\n",
    "#     status = \"✓\" if is_installed(package) else \"✗\"\n",
    "#     print(f\"  {status} {pip_name}\")\n",
    "\n",
    "# import sys\n",
    "# import os\n",
    "# os.chdir(\"/home/azureuser/NLP_jw1123/BestModel\")\n",
    "# print(\"Working directory set to:\", os.getcwd())\n",
    "\n",
    "# packages = [\n",
    "#     'pandas',\n",
    "#     'numpy',\n",
    "#     'matplotlib',\n",
    "#     'nltk',\n",
    "#     'torch==2.6.0',\n",
    "#     'transformers',\n",
    "#     'accelerate',\n",
    "#     'scikit-learn',\n",
    "#     'sentencepiece',\n",
    "#     'datasets',\n",
    "#     'tiktoken',\n",
    "#     'protobuf'\n",
    "# ]\n",
    "\n",
    "# !{sys.executable} -m pip install {' '.join(packages)} --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2bffa2-e045-4c93-b6d0-cded40b411d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    classification_report,\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Reproducibility seed\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6fd27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- All hyperparameters in one place ----\n",
    "CONFIG = {\n",
    "    \"model_name\": \"roberta-large\",\n",
    "    \"max_length\": 256,\n",
    "    \"batch_size\": 8,\n",
    "    \"gradient_accumulation_steps\": 4,  # effective batch size = 32\n",
    "    \"learning_rate\": 1e-5,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"num_epochs\": 10,\n",
    "    \"warmup_ratio\": 0.1,\n",
    "    \"patience\": 5,\n",
    "    \"seed\": SEED,\n",
    "    \"focal_alpha\": 0.83,              # optimal for post-aug ratio 1588:7581 = 1:4.8\n",
    "    \"focal_gamma\": 2.0,\n",
    "    # 1 random-swap copy per PCL sample: 794 → 1588 PCL vs 7581 non-PCL (ratio ~1:4.8)\n",
    "    \"aug_copies\": 1,\n",
    "    \"threshold_lo\": 0.30,\n",
    "    \"threshold_hi\": 0.70,\n",
    "    \"threshold_step\": 0.01,\n",
    "    # LLRD: each lower layer's lr = base_lr * llrd_decay^k (k=1 for top layer, k=n_layers+1 for embeddings)\n",
    "    \"llrd_decay\": 0.9,\n",
    "}\n",
    "\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d70a325",
   "metadata": {},
   "source": [
    "## Stage A: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "288db942",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory set to: /home/azureuser/NLP_jw1123/BestModel\n",
      "Official split par_ids — Train: 8375, Dev: 2094\n",
      "\n",
      "Total labelled samples: 10469\n",
      "PCL:     993 (9.5%)\n",
      "Not PCL: 9476 (90.5%)\n",
      "Test samples: 3832\n"
     ]
    }
   ],
   "source": [
    "# Resolve data path (works from BestModel/ or repo root)\n",
    "DATA_FILE = \"dontpatronizeme_pcl.tsv\"\n",
    "TRAIN_SPLIT_FILE = \"train_semeval_parids-labels.csv\"\n",
    "DEV_SPLIT_FILE = \"dev_semeval_parids-labels.csv\"\n",
    "TEST_FILE = \"task4_test.tsv\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_file(name):\n",
    "    if os.path.exists(name):\n",
    "        return name\n",
    "    if os.path.exists(os.path.join(\"..\", name)):\n",
    "        return os.path.join(\"..\", name)\n",
    "    raise FileNotFoundError(f\"{name} not found in current or parent directory.\")\n",
    "\n",
    "DATA_PATH = find_file(DATA_FILE)\n",
    "TRAIN_SPLIT_PATH = find_file(TRAIN_SPLIT_FILE)\n",
    "DEV_SPLIT_PATH = find_file(DEV_SPLIT_FILE)\n",
    "TEST_PATH = find_file(TEST_FILE)\n",
    "\n",
    "# ---- Load main dataset ----\n",
    "df = pd.read_csv(\n",
    "    DATA_PATH,\n",
    "    sep=\"\\t\",\n",
    "    skiprows=4,\n",
    "    header=None,\n",
    "    names=[\"par_id\", \"art_id\", \"keyword\", \"country_code\", \"text\", \"label\"],\n",
    ")\n",
    "\n",
    "# Binarise: 0-1 -> Not PCL (0), 2-4 -> PCL (1)\n",
    "df[\"binary_label\"] = (df[\"label\"] >= 2).astype(int)\n",
    "df[\"text\"] = df[\"text\"].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "# ---- Load official SemEval train/dev splits ----\n",
    "train_split = pd.read_csv(TRAIN_SPLIT_PATH)\n",
    "dev_split = pd.read_csv(DEV_SPLIT_PATH)\n",
    "print(f\"Official split par_ids — Train: {len(train_split)}, Dev: {len(dev_split)}\")\n",
    "\n",
    "# ---- Load test set ----\n",
    "test_df = pd.read_csv(\n",
    "    TEST_PATH,\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"test_id\", \"art_id\", \"keyword\", \"country_code\", \"text\"],\n",
    ")\n",
    "test_df[\"text\"] = test_df[\"text\"].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "n_total = len(df)\n",
    "n_pcl = df[\"binary_label\"].sum()\n",
    "print(f\"\\nTotal labelled samples: {n_total}\")\n",
    "print(f\"PCL:     {n_pcl} ({n_pcl/n_total*100:.1f}%)\")\n",
    "print(f\"Not PCL: {n_total - n_pcl} ({(n_total - n_pcl)/n_total*100:.1f}%)\")\n",
    "print(f\"Test samples: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ed0dba1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 8375 matched out of 8375 split IDs  (PCL 794)\n",
      "Dev:   2094 matched out of 2094 split IDs  (PCL 199)\n"
     ]
    }
   ],
   "source": [
    "# Use official SemEval train/dev splits (join on par_id)\n",
    "train_par_ids = set(train_split[\"par_id\"].values)\n",
    "dev_par_ids = set(dev_split[\"par_id\"].values)\n",
    "\n",
    "train_df = df[df[\"par_id\"].isin(train_par_ids)].reset_index(drop=True)\n",
    "dev_df = df[df[\"par_id\"].isin(dev_par_ids)].reset_index(drop=True)\n",
    "\n",
    "# Sanity check: ensure all par_ids were matched\n",
    "n_train_matched = len(train_df)\n",
    "n_dev_matched = len(dev_df)\n",
    "print(f\"Train: {n_train_matched} matched out of {len(train_split)} split IDs  (PCL {train_df['binary_label'].sum()})\")\n",
    "print(f\"Dev:   {n_dev_matched} matched out of {len(dev_split)} split IDs  (PCL {dev_df['binary_label'].sum()})\")\n",
    "\n",
    "if n_train_matched != len(train_split) or n_dev_matched != len(dev_split):\n",
    "    print(\"WARNING: Some par_ids from the split files were not found in the dataset!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xrwv01unx3i",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Data Augmentation: random word swap only ----\n",
    "# Synonym replacement was removed: it targets long content words that are\n",
    "# exactly the PCL-distinctive vocabulary (destitute, underprivileged, needy …),\n",
    "# replacing them with generic synonyms that dilute the classification signal.\n",
    "# A single random swap preserves all original words while adding surface-form\n",
    "# variation at zero network/API cost.\n",
    "\n",
    "def augment_text(text, rng=None):\n",
    "    \"\"\"Perform one random word swap for surface-form variation.\"\"\"\n",
    "    if rng is None:\n",
    "        rng = random.Random()\n",
    "    words = text.split()\n",
    "    if len(words) >= 2:\n",
    "        i, j = rng.sample(range(len(words)), 2)\n",
    "        words[i], words[j] = words[j], words[i]\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "# ---- Augment minority (PCL) class in training set only ----\n",
    "_pcl_rows = train_df[train_df[\"binary_label\"] == 1]\n",
    "_aug_rng = random.Random(CONFIG[\"seed\"])\n",
    "\n",
    "aug_rows = []\n",
    "for _, row in _pcl_rows.iterrows():\n",
    "    for _ in range(CONFIG[\"aug_copies\"]):\n",
    "        aug_rows.append({**row.to_dict(), \"text\": augment_text(row[\"text\"], rng=_aug_rng)})\n",
    "\n",
    "train_df = (\n",
    "    pd.concat([train_df, pd.DataFrame(aug_rows)], ignore_index=True)\n",
    "    .sample(frac=1, random_state=CONFIG[\"seed\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "n_pcl   = train_df[\"binary_label\"].sum()\n",
    "n_total = len(train_df)\n",
    "print(f\"After augmentation — total: {n_total}\")\n",
    "print(f\"  PCL:     {n_pcl} ({n_pcl/n_total*100:.1f}%)\")\n",
    "print(f\"  Not PCL: {n_total - n_pcl} ({(n_total - n_pcl)/n_total*100:.1f}%)\")\n",
    "\n",
    "_sample = _pcl_rows.iloc[0][\"text\"]\n",
    "print(f\"\\nOriginal:  {_sample}\")\n",
    "print(f\"Swapped:   {augment_text(_sample, rng=random.Random(CONFIG['seed']))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fac7c7",
   "metadata": {},
   "source": [
    "## Stage B: Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4b9dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Tokenisation ----\n",
    "tokenizer = AutoTokenizer.from_pretrained(CONFIG[\"model_name\"])\n",
    "\n",
    "# Check token length distribution BEFORE choosing max_length\n",
    "_raw_lengths = [\n",
    "    len(tokenizer.encode(t, add_special_tokens=True))\n",
    "    for t in df[\"text\"].tolist()\n",
    "]\n",
    "_lengths = np.array(_raw_lengths)\n",
    "print(\"Token length percentiles (whole dataset):\")\n",
    "for p in [50, 75, 90, 95, 99, 100]:\n",
    "    print(f\"  p{p:3d}: {np.percentile(_lengths, p):.0f} tokens\")\n",
    "print(f\"  Truncated at 128: {(_lengths > 128).sum()} / {len(_lengths)} \"\n",
    "      f\"({(_lengths > 128).mean()*100:.1f}%)\")\n",
    "print(f\"  Truncated at 256: {(_lengths > 256).sum()} / {len(_lengths)} \"\n",
    "      f\"({(_lengths > 256).mean()*100:.1f}%)\")\n",
    "\n",
    "\n",
    "def tokenize(texts, max_length):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"\\nTokenising train...\")\n",
    "train_enc = tokenize(train_df[\"text\"], CONFIG[\"max_length\"])\n",
    "print(\"Tokenising dev...\")\n",
    "dev_enc = tokenize(dev_df[\"text\"], CONFIG[\"max_length\"])\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a0a5da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 524, Dev batches: 131\n"
     ]
    }
   ],
   "source": [
    "class PCLDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"input_ids\": self.encodings[\"input_ids\"][idx],\n",
    "            \"attention_mask\": self.encodings[\"attention_mask\"][idx],\n",
    "            \"labels\": torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "train_dataset = PCLDataset(train_enc, train_df[\"binary_label\"].values)\n",
    "dev_dataset = PCLDataset(dev_enc, dev_df[\"binary_label\"].values)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=True\n",
    ")\n",
    "dev_loader = DataLoader(\n",
    "    dev_dataset, batch_size=CONFIG[\"batch_size\"], shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Dev batches: {len(dev_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cbd7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Focal Loss ----\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    \"\"\"Focal loss for imbalanced binary classification.\n",
    "\n",
    "    FL(p_t) = -alpha_t * (1 - p_t)^gamma * log(p_t)\n",
    "    alpha=0.83 upweights the rare PCL class; gamma=2 focuses on hard examples.\n",
    "    alpha is near-optimal: n_neg/(n_pos+n_neg) = 7581/9169 ≈ 0.83.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha=0.83, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.register_buffer(\"alpha_pos\", torch.tensor(alpha))\n",
    "        self.register_buffer(\"alpha_neg\", torch.tensor(1.0 - alpha))\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        probs = torch.softmax(logits.float(), dim=-1)   # cast to fp32 for numerical stability\n",
    "        p_t = probs.gather(1, targets.unsqueeze(1)).squeeze(1)\n",
    "        alpha_t = torch.where(targets == 1, self.alpha_pos, self.alpha_neg)\n",
    "        focal_weight = alpha_t * (1 - p_t) ** self.gamma\n",
    "        loss = -focal_weight * torch.log(p_t + 1e-8)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "# ---- Model (all layers trainable) ----\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    CONFIG[\"model_name\"], num_labels=2,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = FocalLoss(alpha=CONFIG[\"focal_alpha\"], gamma=CONFIG[\"focal_gamma\"])\n",
    "loss_fn.to(device)\n",
    "print(f\"Using Focal Loss: alpha={CONFIG['focal_alpha']}, gamma={CONFIG['focal_gamma']}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# ---- LLRD Optimiser ----\n",
    "# Classifier head gets the full lr; each lower layer is scaled by decay^k\n",
    "# (k=1 for top layer, k=n_layers+1 for embeddings).\n",
    "# Dynamic: works for both roberta-base (12 layers) and roberta-large (24 layers).\n",
    "_decay    = CONFIG[\"llrd_decay\"]\n",
    "_lr       = CONFIG[\"learning_rate\"]\n",
    "_n_layers = len(model.roberta.encoder.layer)  # 12 for base, 24 for large\n",
    "\n",
    "param_groups = [\n",
    "    {\"params\": model.classifier.parameters(), \"lr\": _lr},\n",
    "]\n",
    "for i, layer in enumerate(reversed(model.roberta.encoder.layer)):\n",
    "    param_groups.append({\"params\": layer.parameters(), \"lr\": _lr * (_decay ** (i + 1))})\n",
    "param_groups.append(\n",
    "    # k = n_layers + 1 so embeddings are always one step below layer 0\n",
    "    {\"params\": model.roberta.embeddings.parameters(), \"lr\": _lr * (_decay ** (_n_layers + 1))}\n",
    ")\n",
    "\n",
    "# Print base LRs BEFORE creating the scheduler.\n",
    "print(f\"\\nLLRD base learning rates (model has {_n_layers} transformer layers):\")\n",
    "group_names = (\n",
    "    [\"classifier\"]\n",
    "    + [f\"layer {_n_layers - 1 - i:>2d}\" for i in range(_n_layers)]\n",
    "    + [\"embeddings\"]\n",
    ")\n",
    "for name, group in zip(group_names, param_groups):\n",
    "    print(f\"  {name}: lr={group['lr']:.2e}\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(param_groups, weight_decay=CONFIG[\"weight_decay\"])\n",
    "\n",
    "# ---- Scheduler ----\n",
    "steps_per_epoch = len(train_loader) // CONFIG[\"gradient_accumulation_steps\"]\n",
    "total_steps = steps_per_epoch * CONFIG[\"num_epochs\"]\n",
    "warmup_steps = int(CONFIG[\"warmup_ratio\"] * total_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "print(f\"\\nTotal optimiser steps: {total_steps}, Warmup steps: {warmup_steps}\")\n",
    "print(f\"Effective batch size: {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c736add",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    \"\"\"Return (avg_loss, f1, probs_array, labels_array).\"\"\"\n",
    "    model.eval()\n",
    "    all_probs, all_labels = [], []\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch in loader:\n",
    "        ids = batch[\"input_ids\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        labs = batch[\"labels\"].to(device)\n",
    "\n",
    "        with autocast():\n",
    "            logits = model(input_ids=ids, attention_mask=mask).logits\n",
    "        running_loss += loss_fn(logits, labs).item()  # loss_fn casts to fp32 internally\n",
    "\n",
    "        probs = torch.softmax(logits.float(), dim=-1)[:, 1]\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "        all_labels.extend(labs.cpu().numpy())\n",
    "\n",
    "    all_probs = np.array(all_probs)\n",
    "    all_labels = np.array(all_labels)\n",
    "    preds = (all_probs >= 0.5).astype(int)\n",
    "    f1 = f1_score(all_labels, preds, pos_label=1)\n",
    "    return running_loss / len(loader), f1, all_probs, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069acc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Training loop with early stopping, gradient accumulation, LLRD & AMP ----\n",
    "scaler = GradScaler()\n",
    "best_f1 = 0.0\n",
    "patience_ctr = 0\n",
    "history = {\"train_loss\": [], \"dev_loss\": [], \"dev_f1\": []}\n",
    "accum_steps = CONFIG[\"gradient_accumulation_steps\"]\n",
    "\n",
    "for epoch in range(1, CONFIG[\"num_epochs\"] + 1):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for step, batch in enumerate(train_loader, 1):\n",
    "        ids = batch[\"input_ids\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        labs = batch[\"labels\"].to(device)\n",
    "\n",
    "        # --- Forward + backward (fp16 via AMP) ---\n",
    "        with autocast():\n",
    "            logits = model(input_ids=ids, attention_mask=mask).logits\n",
    "            loss = loss_fn(logits, labs) / accum_steps\n",
    "        scaler.scale(loss).backward()\n",
    "        epoch_loss += loss.item() * accum_steps\n",
    "\n",
    "        if step % accum_steps == 0 or step == len(train_loader):\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        if step == 1 or step % 10 == 0 or step == len(train_loader):\n",
    "            print(f\"  [ep {epoch}] step {step:>4}/{len(train_loader)}  loss={loss.item() * accum_steps:.4f}\")\n",
    "\n",
    "    avg_train = epoch_loss / len(train_loader)\n",
    "    dev_loss, dev_f1, _, _ = evaluate(model, dev_loader)\n",
    "\n",
    "    history[\"train_loss\"].append(avg_train)\n",
    "    history[\"dev_loss\"].append(dev_loss)\n",
    "    history[\"dev_f1\"].append(dev_f1)\n",
    "\n",
    "    tag = \"\"\n",
    "    if dev_f1 > best_f1:\n",
    "        best_f1 = dev_f1\n",
    "        patience_ctr = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        tag = \" *saved*\"\n",
    "    else:\n",
    "        patience_ctr += 1\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch}: train_loss={avg_train:.4f}  \"\n",
    "        f\"dev_loss={dev_loss:.4f}  dev_f1={dev_f1:.4f}  \"\n",
    "        f\"patience={patience_ctr}/{CONFIG['patience']}{tag}\"\n",
    "    )\n",
    "\n",
    "    if patience_ctr >= CONFIG[\"patience\"]:\n",
    "        print(\"Early stopping.\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nBest dev F1 (t=0.5): {best_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ae979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Training curves ----\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "epochs_range = range(1, len(history[\"train_loss\"]) + 1)\n",
    "ax1.plot(epochs_range, history[\"train_loss\"], \"o-\", label=\"Train\")\n",
    "ax1.plot(epochs_range, history[\"dev_loss\"], \"s-\", label=\"Dev\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_title(\"Loss\")\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(epochs_range, history[\"dev_f1\"], \"o-\", color=\"green\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"F1\")\n",
    "ax2.set_title(\"Dev F1 (Positive Class)\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_curves.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f9f3b5",
   "metadata": {},
   "source": [
    "## Stage C: Threshold Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d820971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload best checkpoint\n",
    "model.load_state_dict(torch.load(\"best_model.pt\", map_location=device))\n",
    "_, _, dev_probs, dev_labels = evaluate(model, dev_loader)\n",
    "\n",
    "# Grid search\n",
    "thresholds = np.arange(\n",
    "    CONFIG[\"threshold_lo\"],\n",
    "    CONFIG[\"threshold_hi\"] + CONFIG[\"threshold_step\"],\n",
    "    CONFIG[\"threshold_step\"],\n",
    ")\n",
    "\n",
    "best_t, best_t_f1 = 0.5, 0.0\n",
    "records = []\n",
    "for t in thresholds:\n",
    "    preds = (dev_probs >= t).astype(int)\n",
    "    f1 = f1_score(dev_labels, preds, pos_label=1)\n",
    "    prec = precision_score(dev_labels, preds, pos_label=1, zero_division=0)\n",
    "    rec = recall_score(dev_labels, preds, pos_label=1, zero_division=0)\n",
    "    records.append({\"t\": t, \"f1\": f1, \"precision\": prec, \"recall\": rec})\n",
    "    if f1 > best_t_f1:\n",
    "        best_t_f1 = f1\n",
    "        best_t = t\n",
    "        \n",
    "\n",
    "res = pd.DataFrame(records)\n",
    "print(f\"Optimal threshold: {best_t:.2f}\")\n",
    "print(f\"F1 at optimal threshold: {best_t_f1:.4f}\")\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(res[\"t\"], res[\"f1\"], label=\"F1\", linewidth=2)\n",
    "ax.plot(res[\"t\"], res[\"precision\"], \"--\", label=\"Precision\", linewidth=2)\n",
    "ax.plot(res[\"t\"], res[\"recall\"], \"--\", label=\"Recall\", linewidth=2)\n",
    "ax.axvline(best_t, color=\"red\", linestyle=\":\", linewidth=2, label=f\"t*={best_t:.2f}\")\n",
    "ax.set_xlabel(\"Threshold\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Threshold Optimisation on Dev Set\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"threshold_optimisation.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e55597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Final evaluation with optimal threshold ----\n",
    "dev_preds = (dev_probs >= best_t).astype(int)\n",
    "\n",
    "print(f\"Threshold = {best_t:.2f}\")\n",
    "print(\"=\" * 55)\n",
    "print(classification_report(dev_labels, dev_preds, target_names=[\"Not PCL\", \"PCL\"]))\n",
    "\n",
    "f1_final = f1_score(dev_labels, dev_preds, pos_label=1)\n",
    "prec_final = precision_score(dev_labels, dev_preds, pos_label=1)\n",
    "rec_final = recall_score(dev_labels, dev_preds, pos_label=1)\n",
    "print(f\"PCL Precision: {prec_final:.4f}\")\n",
    "print(f\"PCL Recall:    {rec_final:.4f}\")\n",
    "print(f\"PCL F1:        {f1_final:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e240bd",
   "metadata": {},
   "source": [
    "## Save Predictions & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5756ccf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- dev.txt: predictions on official dev set ----\n",
    "dev_pred_path = os.path.join(\"..\", \"dev.txt\")\n",
    "with open(dev_pred_path, \"w\") as f:\n",
    "    for p in dev_preds:\n",
    "        f.write(f\"{p}\\n\")\n",
    "print(f\"Saved {len(dev_preds)} dev predictions -> {dev_pred_path}\")\n",
    "\n",
    "# ---- test.txt: predictions on actual unlabelled test set (task4_test.tsv) ----\n",
    "print(f\"\\nGenerating predictions on test set ({len(test_df)} samples)...\")\n",
    "test_enc = tokenize(test_df[\"text\"], CONFIG[\"max_length\"])\n",
    "test_ds = PCLDataset(test_enc, np.zeros(len(test_df), dtype=int))  # dummy labels\n",
    "test_loader = DataLoader(test_ds, batch_size=CONFIG[\"batch_size\"], shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "all_test_probs = []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        ids = batch[\"input_ids\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        logits = model(input_ids=ids, attention_mask=mask).logits\n",
    "        probs = torch.softmax(logits, dim=-1)[:, 1]\n",
    "        all_test_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "test_preds = (np.array(all_test_probs) >= best_t).astype(int)\n",
    "test_pred_path = os.path.join(\"..\", \"test.txt\")\n",
    "with open(test_pred_path, \"w\") as f:\n",
    "    for p in test_preds:\n",
    "        f.write(f\"{p}\\n\")\n",
    "print(f\"Saved {len(test_preds)} test predictions -> {test_pred_path}\")\n",
    "print(f\"Test PCL rate: {test_preds.sum()}/{len(test_preds)} ({test_preds.mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5936b720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Save model & tokenizer for reproducibility ----\n",
    "save_dir = \"saved_model\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "# Save training config + threshold\n",
    "meta = {\n",
    "    **CONFIG,\n",
    "    \"best_threshold\": float(best_t),\n",
    "    \"best_f1\": float(best_t_f1),\n",
    "    \"dev_precision\": float(prec_final),\n",
    "    \"dev_recall\": float(rec_final),\n",
    "}\n",
    "with open(os.path.join(save_dir, \"training_config.json\"), \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(f\"Model saved to {save_dir}/\")\n",
    "print(f\"Best threshold: {best_t:.2f}\")\n",
    "print(f\"Dev F1: {best_t_f1:.4f}\")\n",
    "print(\"\\nDone!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
